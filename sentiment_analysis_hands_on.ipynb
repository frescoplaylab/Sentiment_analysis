{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyasrbhat/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.datasets.imdb import get_word_index\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = \n",
    "word_to_id = \n",
    "word_to_id[\"<PAD>\"] = \n",
    "word_to_id[\"<START>\"] = \n",
    "word_to_id[\"<UNK>\"] = \n",
    "id_to_word = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([id_to_word[i] for i in X_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"not good\", \"climax was awesome !\", \"really liked the movie\", \"too lengthy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('not', 1), ('good', 1), ('climax', 1), ('was', 1), ('awesome', 1), ('really', 1), ('liked', 1), ('the', 1), ('movie', 1), ('too', 1), ('lengthy', 1)])\n",
      "4\n",
      "{'not': 1, 'good': 2, 'climax': 3, 'was': 4, 'awesome': 5, 'really': 6, 'liked': 7, 'the': 8, 'movie': 9, 'too': 10, 'lengthy': 11}\n",
      "{'not': 1, 'good': 1, 'climax': 1, 'was': 1, 'awesome': 1, 'liked': 1, 'really': 1, 'movie': 1, 'the': 1, 'too': 1, 'lengthy': 1}\n"
     ]
    }
   ],
   "source": [
    "t = \n",
    "\n",
    "print(t.word_counts)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence \n",
    "max_review_length = \n",
    "X_train = \n",
    "X_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32 \n",
    "\n",
    "\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 229s 9ms/step - loss: 0.4779 - acc: 0.7670 - val_loss: 0.3565 - val_acc: 0.8524\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 230s 9ms/step - loss: 0.3387 - acc: 0.8597 - val_loss: 0.3613 - val_acc: 0.8454\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.2915 - acc: 0.8795 - val_loss: 0.3252 - val_acc: 0.8679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0dd5e3c18>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really liked the movie and had fun. Sentiment: 0.89158416\n",
      "this movie was terrible and bad. Sentiment: 0.12732758\n"
     ]
    }
   ],
   "source": [
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(np.array([tmp_padded][0]))[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
